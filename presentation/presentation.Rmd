---
title: |
 | \fontsize{18}{15}\sf\bfseries{What makes a good prediction interval or probabilistic forecast?}
 
author: 
- Beinan Xu \newline
- \small (supervisor:Professor Rob Hyndman)
date: "`r format(Sys.time(), '%d %B %Y')`"
fontsize: 14pt
output:
  beamer_presentation:
    fig_height: 5
    fig_width: 8
    highlight: tango
    theme: metropolis
header-includes:
  - \usepackage{MonashBlue}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE, dev.args=list(bg=grey(0.9), pointsize=11))
library(forecast)
library(tidyverse)
library(forcats)
library(scoringRules)
library(lubridate)
library(fGarch)
library(Mcomp)


store_fc_results <- function(u, method)
{
  fc <- u[["x"]] %>% method(h=u[["h"]],level=95)
  fcsd <- c(fc$upper-fc$lower)/2/qnorm(0.975)
  return(cbind(actual=u[["xx"]], mean=c(fc$mean),sd=fcsd))
}

calc_scores <- function(x, scorefn)
{
  scores <- scorefn(y=x[,"actual"], mean=x[,"mean"], sd=x[,"sd"])
  return(mean(scores))
}

PIS<-function(y,l,u,a)
{
  (u-l)+2/a*pmax(0,y-u)+2/a*pmax(0,l-y)
}

```

# Introduction

Statistical forecasting models usually provide an estimate of the forecast distribution, or at least a prediction interval, for each forecast horizon.

## Scoring Rules

* Interval scoring rules
* Distribution scoring rules

## Case study

* Financial data: ASX 200
* M3 datasets

# Scoring Rules


* Proper scoring rules provide summary measures of the predictive performance that allow for the joint assessment of calibration and sharpness.
* The scores to be negatively oriented penalties that forecasts wish to minimize.


# Interval forecast

* Occasionally, full predictive distributions are difficult to specify, and the forecaster might quote predictive quantiles, such as value at risk in financial applications or prediction intervals only.(Gneiting and Raftery, 2007)

* Interval forecasts is a special case of quantile prediction.

# Interval scoring rule

Winkler loss scoring rule is selected to evaluate interval forecasts.

\small

* It is the most commonly used interval forecast loss function.The forecaster is rewarded for narrow prediction intervals, and he or she incurs a penalty, the size of which depends on $\alpha$, if the observation misses the interval.

\small

* $(1-\alpha)\times100\%$ is represent the central prediction interval.

##Winkler loss scoring rules

\small
 $S_\alpha^{int}(l,u;x)=(u-l)+\frac{2}{\alpha}(l-x)1\{x<l\}+\frac{2}{\alpha}(x-u)1\{x>u\}$
 
 


# Probabilistic forecast

\small

* A probabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibra tion, on the basis of the available information set. (Gneiting and Katzfuss, 2014)

# Distribution scoring rules 

\small

Three scoring are chosen to evaluate probabilistic forecasts under Gaussian predictive distribution.

## Logarithmic score
  $$
      LogS(F,y)=logF(y)
  $$


## Continuous Ranked Probability Score
 $$
       CRPS(F,y)=\int_{-\infty}^{\infty}(F(x)-1\left\{y\leq{x}\right\})^2 dx
 $$

## Dawid-Sebastianti score
 \small
  $$
     DSS(F,y)=\frac{(y-\mu_F)^2}{\sigma_F^2}+2log\sigma_F
  $$

# Model selection

## ARIMA by auto.arima

\small

* This function in R uses a variation of the Hyndman Khandakar algorithm (Hyndman and Khandakar 2008), which combines unit root tests, minimization of the AICc and MLE to obtain an ARIMA model.
     
* By setting the model and finding the smallest AICc, to obtain the most suitable model.

## GARCH by fGarch

\small

This model has become important in the analysis of time series data, particularly in financial applications when the goal is to analyze and forecast volatility.


# Model selection

## ETS by ets()

\small

* Information criteria can be used for model selection, AIC and AICc.

* All ETS models are non-stationary, so they cannot be used in financial data.

## Random Walk model by rwf()

\small

* The results after evaluating forecast results by using scoring rules

* Random walk models are widely used for non-stationary data. 

# Case study one: ASX200

* Data information
     + The raw data comes from YahooFinance (2018), it is the daily data over 10 years period
until the beginning of 2018.
     + Features
         + The unconditional distribution is leptokurtic
         + The return series appears to
have a constant unconditional mean
         + The volatility of return changes over time and volatility tends to arrive in clusters

* Evaluating by scoring rules
     + Interval forecasts
     + Probabilistic forecasts


# Evaluating by interval score 

* Models selection
* Evaluation results
    + Interval forecasts by two models
    + Setting different prediction intervals
    + Winkler loss scoring rule

# ARIMA model select
```{r asxdata, include=FALSE}
raw.asx <- read_csv("data/s&p asx 200.csv")
asx <- as.ts(raw.asx[, 6])
dfasx <- diff(asx)

dftrain <- window(dfasx, end = 2800)
dftest <- window(dfasx, start = 2801)
dffit.arima <- auto.arima(dftrain,stepwise = F)
```

\footnotesize
```{r arimaasx, message=FALSE}
dffit.arima
```

# GARCH model select
```{r garch, include=FALSE, dependson="arima"}
garch11_results <- garchFit(~ arma(0, 3) + garch(1, 1), data = dftrain, trace = FALSE)
garch12_results <- garchFit(~ arma(0, 3) + garch(1, 2), data = dftrain, trace = FALSE)
garch21_results <- garchFit(~ arma(0, 3) + garch(2, 1), data = dftrain, trace = FALSE)
garch22_results <- garchFit(~ arma(0, 3) + garch(2, 2), data = dftrain, trace = FALSE)
garch1_results <- garchFit(~ arma(0, 3) + garch(1, 0), data = dftrain, trace = FALSE)
garch2_results <- garchFit(~ arma(0, 3) + garch(2, 0), data = dftrain, trace = FALSE)

garch11_results@fit$ics -> garch11
garch12_results@fit$ics -> garch12
garch21_results@fit$ics -> garch21
garch22_results@fit$ics -> garch22
garch1_results@fit$ics -> arch1
garch2_results@fit$ics -> arch2

garch <- rbind(garch11, garch12, garch21, garch22, arch1, arch2)
```


```{r table1,message=FALSE, dependson="garch"}
knitr::kable(round(garch, 3), caption = "Garch model select", booktabs = TRUE) 
```

# Evaluating interval forecasts for two models

```{r r Infasx, include=FALSE}
## arima 

dffc.arima <- forecast(dffit.arima, h = length(dftest), level=c(1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,99))
dffc.arima 

#loop
c=1:21

b=(100-dffc.arima$level)/100
w<-cbind(c,b)%>% as.matrix()

ASX_PIS.arima<-matrix(0,nrow=21,ncol=1)
for (b in 1:21){
  ASX_PIS.arima[b]<-PIS(as.numeric(dftest),l=(dffc.arima$lower[, w[b,1]]),u=(dffc.arima$upper[, w[b,1]]),a=w[b,2])%>% mean()
}

colnames(ASX_PIS.arima)="Interval scores" 
rownames(ASX_PIS.arima)<-dffc.arima$level
```

```{R plot1, include=FALSE}
data.frame(level=dffc.arima$level, PIS=ASX_PIS.arima[,1]) %>%
  ggplot(aes(x=level, y=PIS)) +
  geom_point() + geom_line()+
  ggtitle("Interval scores for ARIMA model at different prediction level")->AA
```


```{R garchPIS, include=FALSE}
## garch
garch_results <- garchFit(~ arma(0, 3) + garch(1, 1), data = dftrain, trace = FALSE)

ASX_PIS.garch<-matrix(0,nrow=21,ncol=1)
for(b in 1:21){
  fc.garch <- predict(garch_results, n.ahead = length(dftest), crit_val = 1-w[b,2])
  fcmean.garch<- as.numeric(fc.garch$meanForecast)
  fcsd.garch<- as.numeric(fc.garch$standardDeviation)
  upper.garch=fcmean.garch+fcsd.garch*qnorm(1-w[b,2]/2)
  lower.garch=fcmean.garch-fcsd.garch*qnorm(1-w[b,2]/2)
  ASX_PIS.garch[b]<-PIS(as.numeric(dftest),l=lower.garch,u=upper.garch,a=w[b,2])%>%mean()
}

colnames(ASX_PIS.garch)="Interval scores" 
rownames(ASX_PIS.garch)<-dffc.arima$level

```


```{R plot2, include=FALSE}
data.frame(level=dffc.arima$level, PIS=ASX_PIS.garch[,1]) %>%
  ggplot(aes(x=level, y=PIS)) +
  geom_point() + geom_line() +
  ggtitle("Interval scores for Garch model at different prediction level")->GG
```


```{r plot4,message=FALSE}
data.frame(ARIMA=ASX_PIS.arima[,1],
           GARCH=ASX_PIS.garch[,1],level=dffc.arima$level)%>%
  ggplot(aes(x=level,y=PIS))+
  geom_line(aes(y = ARIMA,colour="ARIMA"))+
  geom_line(aes(y = GARCH,colour="GARCH"))+
  ggtitle("Comparing the interval scores of the two models at different level")

 
```

# Evaluation of probablisitic forecasts

* Same models 

* Evaluation results
    + Probabilistic forecasts by two models
    + Three scoring rules

# Result of evaluation

```{r arima, include=FALSE}

# arima
dffit.arima <- auto.arima(dftrain,stepwise = F)
dffc.arima <- forecast(dffit.arima, h = length(dftest))
dffcmean.arima <- as.numeric(dffc.arima$mean)
dffcsd.arima <- as.numeric((dffc.arima$upper[, 2] - dffc.arima$lower[, 2]) / 2 / 1.96)

dfcrps <- crps(as.numeric(dftest), family = "normal", mean = dffcmean.arima, sd = dffcsd.arima) %>% mean() %>% round(digits = 5)
dflogs <- logs_norm(as.numeric(dftest), mean = dffcmean.arima, sd = dffcsd.arima) %>% mean() %>% round(digits = 5)
dfdss <- dss_norm(as.numeric(dftest), mean = dffcmean.arima, sd = dffcsd.arima) %>% mean() %>% round(digits = 5)

ARIMA <- c(dfcrps, dflogs, dfdss)
```



```{r garchfit, include=FALSE, dependson="arima"}
# garch
garch_results <- garchFit(~ arma(0, 3) + garch(1, 1), data = dftrain, trace = FALSE)
garch_results
fc.garch <- predict(garch_results, n.ahead = length(dftest), crit_val = 0.95)

fcmean.garch <- as.numeric(fc.garch$meanForecast)
fcsd.garch <- as.numeric(fc.garch$standardDeviation)

crps <- crps(as.numeric(dftest), family = "normal", mean = fcmean.garch, sd = fcsd.garch) %>% mean() %>% round(digits = 5)
logs <- logs_norm(as.numeric(dftest), mean = fcmean.garch, sd = fcsd.garch) %>% mean() %>% round(digits = 5)
dss <- dss_norm(as.numeric(dftest), mean = fcmean.garch, sd = fcsd.garch) %>% mean() %>% round(digits = 5)

GARCH <- c(crps, logs, dss)
```
```{r table2,message=FALSE, dependson="garchfit"}
ASX <- round(rbind(GARCH, ARIMA),2)
colnames(ASX) <- c("CRPS","LogS","DSS")
rownames(ASX) <- c("GARCH","ARIMA")
  knitr::kable(ASX, caption = "Scoring Rules for MA model and GARCH model", booktabs = TRUE)
```

# Case study two: M3 data set

* The M3 dataset includes 3003 different type time series, it is from R packages “Mcomp” (Hyndman, 2018).


# Evaluating by interval forecast

* Models selection
    + ARIMA and ETS
    
* Standardization 
    + Randon wolk model
    + CRPS scoring rules

# Evaluating interval scores
```{r M3PIS,include=FALSE}

store_ifc_results <- function(u, method)
{
  fc <- u[["x"]] %>% method(h=u[["h"]],level=95)
  return(cbind(actual=u[["xx"]], lower=c(fc$lower),upper=c(fc$upper)))
}

calc_PIS <- function(x, scorefn)
{
  scores <- scorefn(y=x[,"actual"], l=x[,"lower"], u=x[,"upper"],a=0.05)
  return(mean(scores))
}

q <- seq(3003)

M3_PIS <- map(M3[q], function(x) {
  c(x[["sn"]], x[["type"]], x[["period"]])
}) %>%
  do.call(what=rbind.data.frame) %>%
  as_tibble()
colnames(M3_PIS) <- c("id","type","period")

rwifc <- map(M3[q], store_ifc_results, method=rwf)

etsifc <- map(M3[q], store_ifc_results,
             method=function(x, h, level){forecast(ets(x), h=h, level=level)})

arimaifc <- map(M3[q], store_ifc_results,
               method=function(x, h, level){forecast(auto.arima(x), h=h, level=level)})

#Eliminating the impact of data units in the model, I create new PIS by naive forecast
RWFI = map_dbl(rwifc, calc_PIS, scorefn=PIS)

PIS_scores <- mutate(M3_PIS,
                      ETS = map_dbl(etsifc, calc_PIS, scorefn=PIS)/RWFI,
                      ARIMA = map_dbl(arimaifc, calc_PIS, scorefn=PIS)/RWFI
) %>%
  gather(-id, -type, -period, value=PIS, key=model)

PIS_scores
M3ilong<-gather(PIS_scores, -id, -type, -period, -model, key=ScoringMethod, value=Score)
```

```{r boxplotPIS,include=FALSE}
# Produce some boxplots
M3ilong %>%
  mutate(
    period = fct_collapse(period, YEARLY = c("OTHER","YEARLY")),
    period = fct_relevel(period, "YEARLY","QUARTERLY","MONTHLY")
  ) %>%
  ggplot() +
    geom_boxplot(aes(x=model, y=Score, fill=ScoringMethod)) +
    facet_grid(ScoringMethod ~ period, scales="free") +
    guides(fill=FALSE) +
    xlab("Forecasting method") +
    ggtitle("Interval scores for the M3 data") -> I
I

```


```{r boxplotlogPIS,message=FALSE}
# Produce some boxplots on log scale
I + scale_y_log10()

```


# Evaluating by probilistic forecasts

* Models selection
    + ARIMA and ETS
    
* Standardization 
    + Randon wolk model
    + CRPS scoring rules
    
# Evaluating distribution scores

```{r M3DF,include=FALSE}
# Work on subset of data?
#k <- sample(3003, size=100)
k <- seq(3003)

M3_scores <- map(M3[k], function(x) {
  c(x[["sn"]], x[["type"]], x[["period"]])
}) %>%
  do.call(what=rbind.data.frame) %>%
  as_tibble()
colnames(M3_scores) <- c("id","type","period")

rwfc <- map(M3[k], store_fc_results, method=rwf)

etsfc <- map(M3[k], store_fc_results,
             method=function(x, h, level){forecast(ets(x), h=h, level=level)})

arimafc <- map(M3[k], store_fc_results,
               method=function(x, h, level){forecast(auto.arima(x), h=h, level=level)})

#Eliminating the impact of data units in the model for CRPS


RWFD = map_dbl(rwfc, calc_scores, scorefn=crps_norm)

#crps
crps_scores <- mutate(M3_scores,
                      ETS = map_dbl(etsfc, calc_scores, scorefn=crps_norm)/RWFD,
                      ARIMA = map_dbl(arimafc, calc_scores, scorefn=crps_norm)/RWFD
) %>%
  gather(-id, -type, -period, value=CRPS, key=model)

#logs
log_scores <- mutate(M3_scores,
                     #RWF = map_dbl(rwfc, calc_scores, scorefn=logs_norm),
                     ETS = map_dbl(etsfc, calc_scores, scorefn=logs_norm),
                     ARIMA = map_dbl(arimafc, calc_scores, scorefn=logs_norm)
) %>%
  gather(-id, -type, -period, value=LOGS, key=model)
#dss
dss_scores<-mutate(M3_scores,
                   #RWF = map_dbl(rwfc, calc_scores, scorefn=dss_norm),
                   ETS = map_dbl(etsfc, calc_scores, scorefn=dss_norm),
                   ARIMA = map_dbl(arimafc, calc_scores, scorefn=dss_norm)
) %>%
  gather(-id, -type, -period, value=DSS, key=model)


M3_scores1 <- left_join(crps_scores,log_scores)
#M3_scores2<- left_join(dss_scores,lin_scores)
M3_scores<-left_join(M3_scores1,dss_scores)



# Now make it long form
M3long <- gather(M3_scores, -id, -type, -period, -model, key=ScoringMethod, value=Score)

```


```{r boxplotPF,include=FALSE}
# Produce some boxplots
M3long %>%
  mutate(
    period = fct_collapse(period, YEARLY = c("OTHER","YEARLY")),
    period = fct_relevel(period, "YEARLY","QUARTERLY","MONTHLY")
  ) %>%
  ggplot() +
    geom_boxplot(aes(x=model, y=Score, fill=ScoringMethod)) +
    facet_grid(ScoringMethod ~ period, scales="free") +
    guides(fill=FALSE) +
    xlab("Forecasting method") +
    ggtitle("Distribution scores for the M3 data") -> p
p
```



```{r boxplotlogDF,message=FALSE}
# Produce some boxplots on log scale
p + scale_y_log10()
```



# conclusion

* Conclusion

\small

This project has focus on the measures for evaluating prediction intervals and probabilistic forecasts using scoring rules. After using two case study, we compared the evaluation results obtained from a range of statistical models.

* Further discussion

    + The evaluation of probabilistic forecasts of multivariate variables.
    
    + The evaluation in both parametric and nonparametric settings.

#  Q & A
  
  \Large
  
  **Question and Answer**
  
  
  

# Reference

  \scriptsize
  
  * Almutaz, Ibrahim ; Ajbar, Abdelhamid ; Khalid, Yasir ; Ali, Emad Desalination, A probabilistic forecast of water demand for a tourist and desalination dependent city: Case of Mecca, Saudi Arabia, 2012*Peer Reviewed Journal* Vol.294, pp.53-59 
  
  * Edgar C. Merkle, Mark Steyvers (2013) Choosing a Strictly Proper Scoring Rule. *Decision Analysis* 10(4):292-304. 
  
  * Gneiting T, Balabdaoui F, Raftery AE. 2007. Probabilistic forecasts, calibration and sharpness. *J. R. Stat. Soc. B* 67:243-68
  
  * Gneiting, T., & Katzfuss, M. (2014). Probabilistic forecasting. *Annual Review of Statistics and Its Application*, 1(1), 125-151.
  
  * Gneiting T, Raftery AE. 2007. Strictly proper scoring rules, prediction, and estimation. *J. Am. Stat. Assoc.* 102:359-78
  
  * Hersbach, H. (2000), “Decomposition of the Continuous Ranked Probability Score for Ensemble Prediction Systems, *Weather and Forecasting*, 15,559-570.
  
  * Hyndman, R. J. (2018). forecast: Forecasting functions for time series and linear models (R package version 8.3). https://CRAN.R-project.org/package=forecast
  
  * Hyndman, R. J., & Athanasopoulos, G. (2018). Forecasting: principles and practice. 2nd ed., Melbourne, Australia: OTexts. https://OTexts.org/fpp2/
  
 
  
  
# Reference

  \scriptsize 
  
   * Lowe, Rachel ; Coelho, Caio As ; Barcellos, Christovam ; Carvalho, Marilia Sá ; Catão, Rafael De Castro ; Coelho, Giovanini E ; Ramalho, Walter Massa ; Bailey, Trevor C ; Stephenson, David B ; Rodó, Xavier eLife, 2016, *Peer Reviewed Journal* Vol.5 
  
  * Matheson, J. E., and Winkler, R. L. (1976), “Scoring Rules for Continuous Probability Distributions, *Management Science*, 22, 1087-1096.
  
  * Raftery, A. E. (2016). Use and communication of probabilistic forecasts. *Statistical Analysis and Data Mining: The ASA Data Science Journal*, 9(6), 397-410.
  
  * Roulston, M. S., and Smith, L. A. (2002), “Evaluating Probabilistic Forecasts Using Information Theory, *Monthly Weather Review*, 130, 1653-1660.

