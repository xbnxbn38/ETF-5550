---
title: 'What makes a good prediction interval or probabilistic forecast?'
degreetype: 'Masters of Applied Econometrics'
author: 'Beinan Xu'
studentid: 26401746
output: bookdown::pdf_book
header-includes:
        - \usepackage{setspace}\doublespacing
        - \usepackage{chngcntr}
        - \usepackage{url}
        - \counterwithin{figure}{section}
        - \counterwithin{table}{section}
site: bookdown::bookdown_site
link-citations: yes
knit: "bookdown::render_book"
---

<!--
Edit these lines as appropriate.
The actual thesis content is in several Rmd files.

You'll need to edit the _bookdown.yml file to set the order in which you'd like them to appear.

If you have specific LaTeX packages to add, put them in monashthesis.tex.

You will need to ensure you have installed the knitr and bookdown packages for R.

You will also need LaTeX installed on your computer.
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, cache=TRUE)
# Load any R packages you need here
library(forecast)
library(tidyverse)
library(forcats)
library(scoringRules)
library(lubridate)
library(fGarch)
library(Mcomp)

store_fc_results <- function(u, method)
{
  fc <- u[["x"]] %>% method(h=u[["h"]],level=95)
  fcsd <- c(fc$upper-fc$lower)/2/qnorm(0.975)
  return(cbind(actual=u[["xx"]], mean=c(fc$mean),sd=fcsd))
}

calc_scores <- function(x, scorefn)
{
  scores <- scorefn(y=x[,"actual"], mean=x[,"mean"], sd=x[,"sd"])
  return(mean(scores))
}

# Because the interval scoring rule function will be used in calc_scores, I think the the result form of forecast by using PIS is similar with the result form by using the code of crps from R package scoringRules, it means we should not take mean in the code of PIS

#PIS<-function(y,l,u,a)
#{
#  mean((u-l)+2/a*pmax(0,y-u)+2/a*pmax(0,l-y))
#}
PIS<-function(y,l,u,a)
{
  (u-l)+2/a*pmax(0,y-u)+2/a*pmax(0,l-y)
}

```


# Abstract {-}

This thesis is about introducing scoring rules and using it to evaluate interval forecasts and probabilistic forecasts. In the past few decades, the interval forecasts and probabilistic forecasts have a very important development and are attracting more and more attention. More and more organizations and individuals begin to use probability prediction instead of point prediction to carry out the future. However, the traditional evaluation methods of point prediction cannot effectively evaluate the results of probabilistic prediction. Because if we want to evaluate the probability prediction effectively, we should not only evaluate the sharpness of the prediction distribution but also evaluate its calibration. For evaluating the result of interval forecasts and probabilistic forecasts, scoring rules is a very effective method. It can evaluate the sharpness of the prediction of distribution while assessing calibration. In this article, we have used different scoring rules to evaluate the different forecasting result base on different models at the index of ASX 200 and M3 datasets.

<!--chapter:end:index.Rmd-->

# Introduction {#ch:intro}

In this world, people wish to understand the future development of different events. For example, residents want to know tomorrow's weather and temperatures to decide what kind of clothes they need to choose. Securities investors want to know the future price trend of securities in order to formulate a suitable investment portfolio. Unfortunately, it is very difficult for humans to predict the future because uncertainty is a universal feature of this world. Although we have a variety of ways to predict future events, such as building suitable time series models based on past information, then predicting future trends over time, none of these methods provide absolutely accurate future predictions. The limitations are that, first of all, various activities and phenomena in the real world are difficult to be perfectly represented by mathematical models, especially humanistic phenomena such as the purchase of lottery tickets (the outcome of the lottery is random). Although there are some natural phenomena, they have certain rules to follow, such as temperature have seasonal changes, so it is very difficult to establish prediction models for them. Second, human cognition is limited. For the event, people cannot collect and obtain all of the information for the relevant factors. Because of these limitations, using these methods to make predictions must not be absolutely accurate. For now, to accurately forecast future is still a difficult task, but as more and more prediction methods and models are developed, forecasters can use them to get what they want. The following problem is how to evaluate these models because choosing the correct assessment method can effectively compare the accuracy of forecast results by using different models, then the forecaster can obtain the most suitable prediction model. 

In choosing the forecast method, point forecast is the most commonly used. But forecasts should be probabilistic (@GK14) and point estimation gradually transform to distribution estimation (@S75). Therefore, interval forecasts and probabilistic are being used more and more frequently. Probabilistic prediction is a method to forecast future uncertain events and development by generating probability prediction distribution. Base on the available information set, to maximize the sharpness of prediction distribution and subject to calibrate (@GK14). Comparing the point forecasts can produce a single point result, such as predicted a stock price in the next day, probabilistic prediction can supply more information to the forecaster by assigning a probability distribution to each future possible outcome as supplying the probabilistic distribution on different prices on the second day. Obviously, probabilistic forecasting has more obvious advantages than point forecasting, so people begin to use probabilistic forecasts to predict activities rather than using point forecasts in many fields, such as finance, weather, medicine etc. In the @R16 paper, it discussed five potential predictors who have a need for probability forecasts.

For evaluating the accuracy of point forecast results, the common ways are to calculate the forecast errors, scale-dependent errors (as Mean absolute error, Root mean squared error), percentage errors (as Mean absolute percentage error) or scale errors (as the mean absolute scaled error) (@RA18). But for interval forecasts and probabilistic forecasts, the scoring rules are used more generally. At present, scoring rules are mainly used in weather forecasting system. They provide such an assessment by giving a numerical score based on probability and actual observation (@W96) and proper scoring rules encourage the forecaster to conduct careful assessment and honesty (@GR07).  

This thesis introduces the scoring rules in Chapter 2. In this Chapter, probability forecasts, sharpness and calibration is reviewed. Also, interval scoring rules and distribution scoring rules are reviewed separately, and they are applied to evaluate interval forecasts and probability forecasts respectively. Meanwhile, the corresponding formula and derived formula are shown in Chapter 2.1 and 2.2. In the third chapter, after using two different models to fit the financial data, interval forecast and probabilistic forecast are performed separately. Then the prediction results are evaluated by using suitable interval scoring rules and distribute scoring rules. In the same way, in Chapter 4, we select M3 data sets that have more abundant data types, and choose more different models to make forecasts, then to evaluate.  





<!--chapter:end:01-intro.Rmd-->

# Assessing probabilistic forecasts using scoring rules

The traditional prediction method is mainly based on point forecasts, which can provide forecasters with future development trend information under given significant level. But the future is extremely uncertain. It's hard to predict an accurate future through the past information. For example, when watching a football match, if the level of the two teams is very different, we can easily judge that the team is more likely to win, but how many goals is hard to know. At this point, the limitations of point forecasts are reflected. But the probabilistic forecasts can be given a probability distribution for all possible future results so that more information can be obtained to predict the uncertain future. If we can assign a different probability to different results in the game, the fans will be able to judge the result of the match.

There are two important factors to evaluate the results of probabilistic forecasts: calibration and sharpness. The meaning of sharpness refers to the centralization of the predicted distribution and the calibration refers to the statistical consistency between the predicted distribution and the observed value. (@GBR07) They affect the quality of probabilistic forecast. Therefore, to evaluate the calibration and sharpness of probability prediction is an important means to evaluate probability prediction results.


## Property of scoring rules

Assume the result of probabilistic forecasts is $F$, $F \in \cal{F}$ where $\cal{F}$ is a suitable class of CDFs, and $G:\cal{F\times\cdot\cdot\cdot\times F\to F}$ . Then the scoring rule will be $S(F,y)$, where $y \in R$ is the realized outcome.

The scoring rule $S$ is proper relative to the class $\cal{F}$ if $$S(F,G)\geq S(G,G)$$ for all $F,G \in \cal{F}$. Also when $F=G$, the two sides of equation are equal, then it meanings the scoring rules is strictly proper.


## Interval scoring rules
### Different between confident interval and prediction interval

Before discussing interval scoring, we must first understand the difference between confidence interval and prediction interval. These two kinds of intervals are often considered to be the same, but this view is wrong, so they need to be very careful when used. For their differences, @RH13 gave a detailed introduction on his blog. The prediction interval is an interval related to the random variable, and all of the random variable are located in the interval. In contrast, confidence interval is a concept of frequency, which is related to parameters. 

Interval forecasts is a special case of quantile prediction. The $(1-\alpha)\times100\%$ is represent the central prediction interval. $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles are upper and lower endpoints (@GBR07).

### Winkler loss scoring rules

The most commonly  used intervel scoring rules is Winkler Loss scoring rules, it was proposed by @W72. 

$$
  S_\alpha^{int}(l,u;x)=(u-l)+\frac{2}{\alpha}(l-x)1\{x<l\}+\frac{2}{\alpha}(x-u)1\{x>u\}
$$
where l and u represent for the quoted $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles.

Following the formula of interval score, the score mainly based on the results of interval forecasts at different conditional level. Therefore it has a wide range of applications and is suitable for different models. 


### The prescriptive optimal interval forecast

This interval scoring rules was proposed by @RDSS18. A event betweena forecaster F and adversary A, where F chooses d and A chooses a scalar $\delta\in[-\infty,0]$. Then othan the formula:

$$
  S^{int}(y,d,\delta;\alpha)=|d|+\delta(1\{y\in{d}\}-(1-\alpha))
$$
where $d=[d^l,d^u]$ with length $|d|=d^u-d^l$.

## Distribution scoring rules

Scoring rules supply the summary measures to evaluate probabilistic forecasts, it assigns a numerical score under the predictive distribution and the events that need to be predicted. (@GBR07) The function of scoring rules is to evaluate the calibration and the sharpness of the forecast distribution results at the same time, then evaluating the quality of probabilistic forecasts. For the results of produced scores, forecasters wish it can be minimized. 

For variables on a continuous sample space, the most commonly used scoring rules are the logarithmic score (LogS), continuous ranked probability score (CRPS) and Dawid-Sebastiani score (DDS). They can be applied effectively to density forecasts. 

### Logarithmic score

For the scoring rules for evaluating probabilistic forecasts, the of the most commonly used rules is the Logarithmic score (logS). It was first proposed by @G52. It is a modified version of relative entropy and can be calculated for real forecasts and realizations. (@RS02) It is a strictly proper scoring rule. But if the prediction is continuous, using ignorance is troublesome (@P10). Despite its shortcomings, it can directly evaluate the results through the forecast model. Therefore, the logarithmic scoring rule can be used in many scenarios and is not limited to specific models. 

The formula is:
 $$
      LogS(F,y)=logF(y)
  $$
For this report, we use the scoring rules to evaluation the probabilistic forecasts under Gaussian predictive distributions. Then the formula of the logarithmic score can be rewritten as below.
  $$
      LogS(N(\mu,\sigma^2),y)=\frac{(y-\mu)^2}{2\sigma^2}+log\sigma+\frac{1}{2}log2\pi
  $$

### Continuous Ranked Probability Score

It is generally considered that it is unrealistic to limit the density forecasts. In the absence of restriction on density forecasts, the CRPS can define scoring rules directly in terms of predictive cumulative distribution functions. It focuses on observing the whole of forecast distributions rather than the special points in these distributions. It can use deterministic values to evaluate the results of probabilistic forecasts. Also, comparing with the CRPS, logarithmic score is a local strictly proper scoring rule. Therefore, there are not many restrictions on its use. 

The formula of continuous ranked probability Score:

   \[
       CRPS(F,y)=\int_{-\infty}^{\infty}(F(x)-1\left\{y\leq{x}\right\})^2 dx
   \]

  \[
    = E_F|Y-y|-\frac{1}{2}E_F|Y-Y'|
  \]
where Y and Y' are independent random variables with CDF F and finite first moment (@GR07). The CPRS can compare the probabilistic forecasts and point forecasts because when the CRPS drop to the absolute error, the probabilistic forecast is a point forecast. (@GK14)

Also, when evaluating probabilistic forecasts under Gaussian predictive distribution the form will re-write:

   \[
       CRPS(N(\mu,\sigma^2),y)=\sigma\left(\frac{y-\mu}{\sigma}\left(2\Phi\left(\frac{y-\mu}{\sigma}\right)-1\right)+2\varphi\left(\frac{y-\mu}{\sigma}\right)-\frac{1}{\sqrt{\pi}}\right)
  \]

### Dawid-Sebastianti score

The CRPS can be easy to understand and convenient to use, but it has a limitation. It can be hard to compute for complex forecast distributions. (@GK14). Therefore, Therefore, when we need to evaluate the probabilistic forecasts under the complex distribution, choosing Dawid-Sebastiani score is a viable alternative. 

The formula of DSS
  \[
     DSS(F,y)=\frac{(y-\mu_F)^2}{\sigma_F^2}+2log\sigma_F
  \]


<!--chapter:end:02-scoringrules.Rmd-->

# Case study one: ASX 200

The ASX 200 is an index on the Australian Securities Exchange officially released on 31st March 2000. It uses market-weighted average calculations based on the 200 largest listed stocks in Australia. These stocks currently account for the Australian stock market value of 82%. It is considered to be the most important index to measure the operation of the Australian stock market.

The data from ASX 200 is a kind of financial time series, it has the following characteristics. Firstly, The unconditional distribution is leptokurtic, it means that comparing with Gaussian distribution it has a high peak and heavy tails. Also, the return series appears to have a constant unconditional mean, so the time series of return might be stationary, some trend models are not suitable to use for financial times series as ETS model. Because of the volatility of return changes over time and volatility tends to arrive in clusters, the GARCH model is very suitable for use. We choose to use ARIMA model and ARIMA-GARCH model to fit model. Also, using these two models can be very intuitively and clearly to compare the results of forecasting and scoring. 

The raw data comes from @YH, it is the daily data over 10 years period until the beginning of 2018. Because of the features of financial time series, we processed data and obtain its simple return. In order to facilitate the final assessment, we have set the data before 2017 as train data, the data for 2017 as test data. Then using train data to select suitable ARIMA model and GARCH model to make a forecast. For the final results, then to evaluate interval forecasts and probabilistic forecasts by using the interval scoring rules and distribution scoring rules respectively. 

```{r asxdata, include=FALSE}
raw.asx <- read_csv("data/s&p asx 200.csv")
asx <- as.ts(raw.asx[, 6])
dfasx <- diff(asx)

dftrain <- window(dfasx, end = 2800)
dftest <- window(dfasx, start = 2801)
```

## Select suitable models

In order to predict data correctly, we should select suitable models firstly. For select ARIMA model, one simple way is to use auto. arima code, it is formed from R package "forecast" by @RH181. This result shows that this ARIMA(0,0,3) is the most suitable for train set of ASX 200. 

```{r modelselect1, message=FALSE}
dffit.arima <- auto.arima(dftrain,stepwise = F)
knitr::kable(dffit.arima$coef, caption = "ARIMA model select", booktabs = TRUE)
```

Follow the results obtained above, we can find a suitable GARCH model by using R package "fGrach" (@WD17). 

```{r garch, include=FALSE, dependson="arima"}
garch11_results <- garchFit(~ arma(0, 3) + garch(1, 1), data = dftrain, trace = FALSE)
garch12_results <- garchFit(~ arma(0, 3) + garch(1, 2), data = dftrain, trace = FALSE)
garch21_results <- garchFit(~ arma(0, 3) + garch(2, 1), data = dftrain, trace = FALSE)
garch22_results <- garchFit(~ arma(0, 3) + garch(2, 2), data = dftrain, trace = FALSE)
garch1_results <- garchFit(~ arma(0, 3) + garch(1, 0), data = dftrain, trace = FALSE)
garch2_results <- garchFit(~ arma(0, 3) + garch(2, 0), data = dftrain, trace = FALSE)

garch11_results@fit$ics -> garch11
garch12_results@fit$ics -> garch12
garch21_results@fit$ics -> garch21
garch22_results@fit$ics -> garch22
garch1_results@fit$ics -> arch1
garch2_results@fit$ics -> arch2

garch <- rbind(garch11, garch12, garch21, garch22, arch1, arch2)
```

```{r table1,message=FALSE, dependson="garch"}
knitr::kable(round(garch, 3), caption = "Garch model select", booktabs = TRUE) 
```

After comparing the AIC of each Garch model, the AIC of the MA(3)-Garch(1,1) is 10.608, it is the smaller than other models. Therefore, The MA(3)-Garch(1,1) is considered the most suitable model for the train set. 

## Interval forecast for the ASX 200 index

After obtaining the most suitable ARIMA model and GARCH model, we first evaluate the results of the ARIMA model by interval scoring rules. By setting 21 different prediction intervals $(1-\alpha)\times100\%$ from 1% to 99%, the results of interval forecasts at different prediction interval level are obtained. After evaluating by interval scoring rules, the change of evaluation score with the extension of the confidence interval can be shown. 

```{r r Infasx, include=FALSE}
## arima 

dffc.arima <- forecast(dffit.arima, h = length(dftest), level=c(1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,99))
dffc.arima 

#loop
c=1:21

b=(100-dffc.arima$level)/100
w<-cbind(c,b)%>% as.matrix()

ASX_PIS.arima<-matrix(0,nrow=21,ncol=1)
for (b in 1:21){
  ASX_PIS.arima[b]<-PIS(as.numeric(dftest),l=(dffc.arima$lower[, w[b,1]]),u=(dffc.arima$upper[, w[b,1]]),a=w[b,2])%>% mean()
}

colnames(ASX_PIS.arima)="Interval scores" 
rownames(ASX_PIS.arima)<-dffc.arima$level
```

```{R plot1, message=FALSE}
data.frame(level=dffc.arima$level, PIS=ASX_PIS.arima[,1]) %>%
  ggplot(aes(x=level, y=PIS)) +
  geom_point() + geom_line()+
  ggtitle("Interval scores for ARIMA model at different confident level")
```

The upper curve was generated by processing the data. This graph shows that as the confidence interval increases, the interval score shows a trend of accelerating and increasing, and the score reaches the highest at 99%. The lower the score represents the better result of the interval forecasts, so the information from this curve shows that the interval forecasts for the simple return of the ASX200 by using ARIMA model are better when the prediction interval level is smaller 

Then use the same way to set prediction intervals, and use GARCH model to forecast the intervals at a different prediction level. After using interval scoring rules to evaluate the results and making graph. After that, a score change curve is obtained, which shows the result very similar to that obtained by using the ARIMA model before. 

```{R garchPIS, include=FALSE}
## garch
garch_results <- garchFit(~ arma(0, 3) + garch(1, 1), data = dftrain, trace = FALSE)

ASX_PIS.garch<-matrix(0,nrow=21,ncol=1)
for(b in 1:21){
  fc.garch <- predict(garch_results, n.ahead = length(dftest), crit_val = 1-w[b,2])
  fcmean.garch<- as.numeric(fc.garch$meanForecast)
  fcsd.garch<- as.numeric(fc.garch$standardDeviation)
  upper.garch=fcmean.garch+fcsd.garch*qnorm(1-w[b,2]/2)
  lower.garch=fcmean.garch-fcsd.garch*qnorm(1-w[b,2]/2)
  ASX_PIS.garch[b]<-PIS(as.numeric(dftest),l=lower.garch,u=upper.garch,a=w[b,2])%>%mean()
}

colnames(ASX_PIS.garch)="Interval scores" 
rownames(ASX_PIS.garch)<-dffc.arima$level

```

```{R plot2, message=FALSE}
data.frame(level=dffc.arima$level, PIS=ASX_PIS.garch[,1]) %>%
  ggplot(aes(x=level, y=PIS)) +
  geom_point() + geom_line() +
  ggtitle("Interval scores for Garch model at different confident level")
```

This graph also shows that as the prediction interval expands, the score increases. It means that the smaller prediction intervals show better score results by using GRACH model. Because the images produced by using the two different models are extremely similar, we put them together for comparison.

```{r plot3,message=FALSE}
data.frame(ARIMA=ASX_PIS.arima[,1],
           GARCH=ASX_PIS.garch[,1],level=dffc.arima$level)%>%
  ggplot(aes(x=level,y=PIS))+
  geom_line(aes(y = ARIMA,colour="ARIMA"))+
  geom_line(aes(y = GARCH,colour="GARCH"))+
  ggtitle("Comparing the interval scores of the two models at different level")

 
```

By comparing these two curves, their overall characteristics are extremely similar. Their scores are not very different at smaller confidence intervals, but in the larger prediction interval, the scores are slightly different by using these two different models. The interval scores of interval forecasts by using GRACH model is becoming smaller than that by using ARIMA model as the prediction interval expands. So, at high confident interval level, interval forecasts by using GARCH model has a relatively good performance. This result illustrates, for the interval forecast of financial return time series, GARCH model can provide the more efficient result to forecasters. And it is also proving that it is more suitable for fitting financial data. 



## Probabilistic forecasts for the ASX 200 index

For probabilistic forecasts, we still using the ARIMA(0,0,3) model and MA(3)-GARCH(1,1) model to fit data and make a forecast, which was produced at Chapter 3.1. Afterward, the train set is also predicted by two different models. However, unlike the result obtained in 3.2, we do not need to set the confidence interval. Instead, use packages R packages "scoringRules" (@JKL17), we can obtain the evaluation results by using three distribution scoring rules (Logarithmic score, Continuous Ranked Probability Score and Dawid-Sebastianti score) directly. 

```{r arima, include=FALSE}

# arima
dffit.arima <- auto.arima(dftrain,stepwise = F)
dffc.arima <- forecast(dffit.arima, h = length(dftest))
dffcmean.arima <- as.numeric(dffc.arima$mean)
dffcsd.arima <- as.numeric((dffc.arima$upper[, 2] - dffc.arima$lower[, 2]) / 2 / 1.96)

dfcrps <- crps(as.numeric(dftest), family = "normal", mean = dffcmean.arima, sd = dffcsd.arima) %>% mean() %>% round(digits = 5)
dflogs <- logs_norm(as.numeric(dftest), mean = dffcmean.arima, sd = dffcsd.arima) %>% mean() %>% round(digits = 5)
dfdss <- dss_norm(as.numeric(dftest), mean = dffcmean.arima, sd = dffcsd.arima) %>% mean() %>% round(digits = 5)

ARIMA <- c(dfcrps, dflogs, dfdss)
```



```{r garchfit, include=FALSE, dependson="arima"}
# garch
garch_results <- garchFit(~ arma(0, 3) + garch(1, 1), data = dftrain, trace = FALSE)
garch_results
fc.garch <- predict(garch_results, n.ahead = length(dftest), crit_val = 0.95)

fcmean.garch <- as.numeric(fc.garch$meanForecast)
fcsd.garch <- as.numeric(fc.garch$standardDeviation)

crps <- crps(as.numeric(dftest), family = "normal", mean = fcmean.garch, sd = fcsd.garch) %>% mean() %>% round(digits = 5)
logs <- logs_norm(as.numeric(dftest), mean = fcmean.garch, sd = fcsd.garch) %>% mean() %>% round(digits = 5)
dss <- dss_norm(as.numeric(dftest), mean = fcmean.garch, sd = fcsd.garch) %>% mean() %>% round(digits = 5)

GARCH <- c(crps, logs, dss)
```

```{r table2,message=FALSE, dependson="garchfit"}
ASX <- round(rbind(GARCH, ARIMA),2)
colnames(ASX) <- c("CRPS","LogS","DSS")
rownames(ASX) <- c("GARCH","ARIMA")
  knitr::kable(ASX, caption = "Scoring Rules for MA model and GARCH model", booktabs = TRUE)
```

According to the table above, the results of three type scoring rules of MA(3)-garch(1,1) model are all smaller than the result of MA(3) Model. Therefore, it can be shown here that the garch model has a better prediction performance compared to MA(3).

<!--chapter:end:03-casestudy1.Rmd-->

# Case study two: M3 datasets 

The M3 dataset includes 3003 different type time series, it is from R packages "Mcomp" (@RH182), it can provide more information for evaluating probabilistic forecast by using scoring rule. For each time series, there are a train set and a test set, which can be easily used to build each forecast models, then predicting and scoring. Different from previous financial data, M3 datasets can use different models for predictive analysis at the same time. In this part, three prediction models are chosen, ARIMA model, ETS model, and Random walk model.

As in the previous chapter, before we start forecasting and evaluating, the suitable models should be selected. However, for the M3 datasets, there are more than 3000 different time series, so we have modeled all the time series separately by these three models. 

Because these different time series come from different fields, their units are all different. Therefore, before the final evaluation process, the units of each different time series should be unified, which can reduce unnecessary errors when comparing the results of the final evaluation. In order to solve this problem, we standardized the results of each grading. The solution is, using the scores of each forecast result from different models by using different scoring rules to divide the scores from the same model by using same scoring rules, then the standardized results are obtained, and the impact of the unit on the final comparison can be removed. 



## Interval forecast for the M3 competition data 

Like the case study one, we first make interval forecasts for every times series from M3 datasets by using three different model, ARIMA model, ETS model and random walk model. Meanwhile, in order to eliminate the impact of units from each time series, the interval forecasts by using average method are also produced.  

Since there are more than 3000 time series, it is inefficient to find four different models for each time sequence, and use the unified automatic program codes from R package "forecasts" by @RH181 to choose the models. Use the train set of each time series to find the most suitable model, and calculate the interval score through the forecasts results and test sets. After that, each interval score from all time series by ARIMA model, ETS model, and Random walk model is divided into the result of the average method to standardize and remove the influence of different units. 


The interval score we use here is the same as the third chapter, using the YY scoring rule.

```{r M3PIS,include=FALSE}

store_ifc_results <- function(u, method)
{
  fc <- u[["x"]] %>% method(h=u[["h"]],level=95)
  return(cbind(actual=u[["xx"]], lower=c(fc$lower),upper=c(fc$upper)))
}

calc_PIS <- function(x, scorefn)
{
  scores <- scorefn(y=x[,"actual"], l=x[,"lower"], u=x[,"upper"],a=0.05)
  return(mean(scores))
}

q <- seq(3003)

M3_PIS <- map(M3[q], function(x) {
  c(x[["sn"]], x[["type"]], x[["period"]])
}) %>%
  do.call(what=rbind.data.frame) %>%
  as_tibble()
colnames(M3_PIS) <- c("id","type","period")

rwifc <- map(M3[q], store_ifc_results, method=rwf)

etsifc <- map(M3[q], store_ifc_results,
             method=function(x, h, level){forecast(ets(x), h=h, level=level)})

arimaifc <- map(M3[q], store_ifc_results,
               method=function(x, h, level){forecast(auto.arima(x), h=h, level=level)})

#Eliminating the impact of data units in the model, I create new PIS by mean forecast

meanfc<-map(M3[q], store_ifc_results, method=meanf)
mean_PIS<-map_dbl(meanfc, calc_PIS, scorefn=PIS)

PIS_scores <- mutate(M3_PIS,
                      RWF = map_dbl(rwifc, calc_PIS, scorefn=PIS)/mean_PIS,
                      ETS = map_dbl(etsifc, calc_PIS, scorefn=PIS)/mean_PIS,
                      ARIMA = map_dbl(arimaifc, calc_PIS, scorefn=PIS)/mean_PIS
) %>%
  gather(-id, -type, -period, value=PIS, key=model)

PIS_scores
M3ilong<-gather(PIS_scores, -id, -type, -period, -model, key=ScoringMethod, value=Score)
```

```{r boxplotPIS,message=FALSE,fig.height=10,fig.width=10, out.width="100%", dependson="M3"}
# Produce some boxplots
M3ilong %>%
  mutate(
    period = fct_collapse(period, YEARLY = c("OTHER","YEARLY")),
    period = fct_relevel(period, "YEARLY","QUARTERLY","MONTHLY")
  ) %>%
  ggplot() +
    geom_boxplot(aes(x=model, y=Score, fill=ScoringMethod)) +
    facet_grid(ScoringMethod ~ period, scales="free") +
    guides(fill=FALSE) +
    xlab("Forecasting method") +
    ggtitle("Scores for the M3 data") -> I
I
```

```{r boxplotlogPIS,message=FALSE,fig.height=10,fig.width=10, out.width="100%", dependson="boxplot"}
# Produce some boxplots on log scale
I + scale_y_log10()
```

## Probabilistic forecasts for the M3 competition data

In this part, three prediction models (ARIMA model, ETS model, and Random walk model) still be used as before. After separately predicting these 3003 different time series, we reached 9009 forecast sets. Then each of these forecasting sets is evaluated by three different scoring rules separately. And to average the evaluation results for each different time series. Use these evaluation results to generate three boxplots, they represent the performance of different models to predict under different scoring rules.

Because of the data should be standardized for CRPS scoring rules, so we use the z-scores standardization to transform the data form M3 data sets. And the LogS and DSS scoring are already transformed, so we should not standards the data. 

```{r M3,include=FALSE}
# Work on subset of data?
#k <- sample(3003, size=100)
k <- seq(3003)

M3_scores <- map(M3[k], function(x) {
  c(x[["sn"]], x[["type"]], x[["period"]])
}) %>%
  do.call(what=rbind.data.frame) %>%
  as_tibble()
colnames(M3_scores) <- c("id","type","period")

rwfc <- map(M3[k], store_fc_results, method=rwf)

etsfc <- map(M3[k], store_fc_results,
             method=function(x, h, level){forecast(ets(x), h=h, level=level)})

arimafc <- map(M3[k], store_fc_results,
               method=function(x, h, level){forecast(auto.arima(x), h=h, level=level)})

#Eliminating the impact of data units in the model for CRPS

calc_crps <- function(x, scorefn)
{
  scores<- (scorefn(y=x[,"actual"], mean=x[,"mean"], sd=x[,"sd"])-x[,"mean"])/x[,"sd"]
  return(mean(scores))
}

#crps
crps_scores <- mutate(M3_scores,
                      RWF = map_dbl(rwfc, calc_crps, scorefn=crps_norm),
                      ETS = map_dbl(etsfc, calc_crps, scorefn=crps_norm),
                      ARIMA = map_dbl(arimafc, calc_crps, scorefn=crps_norm)
) %>%
  gather(-id, -type, -period, value=CRPS, key=model)

#logs
log_scores <- mutate(M3_scores,
                     RWF = map_dbl(rwfc, calc_scores, scorefn=logs_norm),
                     ETS = map_dbl(etsfc, calc_scores, scorefn=logs_norm),
                     ARIMA = map_dbl(arimafc, calc_scores, scorefn=logs_norm)
) %>%
  gather(-id, -type, -period, value=LOGS, key=model)
#dss
dss_scores<-mutate(M3_scores,
                   RWF = map_dbl(rwfc, calc_scores, scorefn=dss_norm),
                   ETS = map_dbl(etsfc, calc_scores, scorefn=dss_norm),
                   ARIMA = map_dbl(arimafc, calc_scores, scorefn=dss_norm)
) %>%
  gather(-id, -type, -period, value=DSS, key=model)


M3_scores1 <- left_join(crps_scores,log_scores)
#M3_scores2<- left_join(dss_scores,lin_scores)
M3_scores<-left_join(M3_scores1,dss_scores)



# Now make it long form
M3long <- gather(M3_scores, -id, -type, -period, -model, key=ScoringMethod, value=Score)

```

```{r boxplot,message=FALSE,fig.height=10,fig.width=10, out.width="100%", dependson="M3"}
# Produce some boxplots
M3long %>%
  mutate(
    period = fct_collapse(period, YEARLY = c("OTHER","YEARLY")),
    period = fct_relevel(period, "YEARLY","QUARTERLY","MONTHLY")
  ) %>%
  ggplot() +
    geom_boxplot(aes(x=model, y=Score, fill=ScoringMethod)) +
    facet_grid(ScoringMethod ~ period, scales="free") +
    guides(fill=FALSE) +
    xlab("Forecasting method") +
    ggtitle("Scores for the M3 data") -> p
p
```


```{r boxplotlog,message=FALSE,fig.height=10,fig.width=10, out.width="100%", dependson="boxplot"}
# Produce some boxplots on log scale
p + scale_y_log10()
```

Although it cannot be known from the above figure how many outliers are generated base on the different forecast model by different scoring rules, boxplots can show 5th, 25th, 50th, 75th and 95th percentiles of central prediction interval width. The width of the obvious random walk model is much narrower than that of other models, which means that its sharpness is sharpest, and the calibration is more accurate, although its mean value is not the lowest. Therefore, in the case of using M3 data sets, the quality of probability predictions derived from random walk model is even higher. This also proves that using scoring rules can simultaneously evaluate the sharpness and calibration of probabilistic results.




<!--chapter:end:04-casestudy2.Rmd-->

# Conclusion

In this report, we introduced what is the probabilistic forecasts, and calibration and sharpness. It also introduced scoring rules. For the three commonly used scoring rules, we show their original formulas and form under Gaussian predictive distribution. In the section of the case study, we first used two models to do probabilistic forecasts for the ASX200 index, to evaluate the forecasts results by using scoring rules. Then we learned how to use scoring rules to evaluate the outcome of forecasts. In the second case study, we used the M3 datasets and used multiple models to do probabilistic forecasts. We learned how the scoring rules evaluated both the probabilities and the results of car calibration and sharpness.

<!--chapter:end:05-conclusion.Rmd-->

