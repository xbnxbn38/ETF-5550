---
title: 'What makes a good prediction interval or probabilistic forecast?'
degreetype: 'Masters of Applied Econometrics'
author: 'Beinan Xu'
studentid: 26401746
output: bookdown::pdf_book
header-includes:
        - \usepackage{setspace}\doublespacing
        - \usepackage{placeins}
        - \usepackage{chngcntr}
        - \usepackage{url}
        - \counterwithin{figure}{section}
        - \counterwithin{table}{section}
site: bookdown::bookdown_site
link-citations: yes
knit: "bookdown::render_book"
---

<!--
Edit these lines as appropriate.
The actual thesis content is in several Rmd files.

You'll need to edit the _bookdown.yml file to set the order in which you'd like them to appear.

If you have specific LaTeX packages to add, put them in monashthesis.tex.

You will need to ensure you have installed the knitr and bookdown packages for R.

You will also need LaTeX installed on your computer.
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, cache=TRUE)
# Load any R packages you need here
library(forecast)
library(tidyverse)
library(forcats)
library(scoringRules)
library(lubridate)
library(fGarch)
library(Mcomp)

store_fc_results <- function(u, method)
{
  fc <- u[["x"]] %>% method(h=u[["h"]],level=95)
  fcsd <- c(fc$upper-fc$lower)/2/qnorm(0.975)
  return(cbind(actual=u[["xx"]], mean=c(fc$mean),sd=fcsd))
}

calc_scores <- function(x, scorefn)
{
  scores <- scorefn(y=x[,"actual"], mean=x[,"mean"], sd=x[,"sd"])
  return(mean(scores))
}
```


# Abstract {-}

This report is about introducing scoring rules and using it to evaluate the results of probabilistic forecasts. In the past few decades, probabilistic forecasts have a very important development and are attracting more and more attention. More and more organizations and individuals begin to use probability prediction instead of point prediction to carry out the future. However, the traditional evaluation methods of point prediction cannot effectively evaluate the results of probabilistic prediction. Because if we want to evaluate the probability prediction effectively, we should not only evaluate the sharpness of the prediction distribution but also evaluate its calibration. For evaluating the result of probabilistic forecasts, scoring rules is a very effective method. It can evaluate the sharpness of the prediction of distribution while assessing calibration. In this article, we have used different scoring rules to evaluate the different forecasting result base on different models at the index of ASX 200 and M3 datasets.
